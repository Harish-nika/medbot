{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install streamlit faiss-cpu numpy sentence-transformers pymupdf openai pickle-mixin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#genrate and store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 162/162 [02:47<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 167.76 seconds\n",
      "Embeddings saved for Current_Essentials_of_Medicine.pdf\n",
      "Text chunks saved for Current_Essentials_of_Medicine.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:53<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 54.02 seconds\n",
      "Embeddings saved for MedicalDiagnosis_and_Treatment_Methods_in_Basic_Medical_Sciences.pdf\n",
      "Text chunks saved for MedicalDiagnosis_and_Treatment_Methods_in_Basic_Medical_Sciences.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  #chunk overlap,chunk size\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Set directory to save embeddings and text chunks\n",
    "EMBEDDING_DIR = \"/home/harish/Agentic_AI/embeddings\"\n",
    "TEXT_CHUNKS_DIR = \"/home/harish/Agentic_AI/text_chunks\"  # Directory for text chunks\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "# Load SBERT model\n",
    "def load_embedding_model():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the model before running the code\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "# Function to generate embeddings using SBERT (batch processing)\n",
    "def get_embeddings_batch(texts):\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True, batch_size=16)  # Batch processing\n",
    "    return embeddings\n",
    "\n",
    "# Function to improve chunking based on topics and subtopics (headings)\n",
    "def split_text_by_headings(text):\n",
    "    heading_pattern = re.compile(r\"^[A-Z][A-Za-z0-9\\s\\-]+:$\")  # Simple heading pattern (e.g., \"Introduction:\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        if heading_pattern.match(line):  # If a heading is detected, start a new chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = [line]  # Start new chunk with heading\n",
    "        else:\n",
    "            current_chunk.append(line)  # Add line to current chunk\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))  # Add remaining text\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to process the PDF files\n",
    "def process_pdf(file_path):\n",
    "    # Read the PDF\n",
    "    with open(file_path, 'rb') as f:\n",
    "        text = extract_text_from_pdf(f.read())\n",
    "    \n",
    "    # Split the text based on headings or topics/subtopics\n",
    "    chunks = split_text_by_headings(text)\n",
    "    \n",
    "    # If chunks are too large, further split them using RecursiveCharacterTextSplitter\n",
    "    all_chunks = []\n",
    "    for chunk in chunks:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        sub_chunks = text_splitter.split_text(chunk)\n",
    "        all_chunks.extend(sub_chunks)\n",
    "    \n",
    "    # Batch processing of embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = get_embeddings_batch(all_chunks)\n",
    "    print(f\"Embedding generation took {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Save embeddings to .npy file\n",
    "    save_path = os.path.join(EMBEDDING_DIR, f\"{os.path.basename(file_path)}.npy\")\n",
    "    np.save(save_path, embeddings)\n",
    "    print(f\"Embeddings saved for {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Save text chunks to .pkl file\n",
    "    text_chunks_path = os.path.join(TEXT_CHUNKS_DIR, f\"{os.path.basename(file_path)}_chunks.pkl\")\n",
    "    with open(text_chunks_path, 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"Text chunks saved for {os.path.basename(file_path)}\")\n",
    "\n",
    "# Process PDF files (adjust the paths to the PDFs)\n",
    "pdf_files = [\"/home/harish/Agentic_AI/books/Current_Essentials_of_Medicine.pdf\", \"/home/harish/Agentic_AI/books/MedicalDiagnosis_and_Treatment_Methods_in_Basic_Medical_Sciences.pdf\"]  # Example PDF file paths\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in ./medbotenv/lib/python3.10/site-packages (1.41.1)\n",
      "Requirement already satisfied: faiss-cpu in ./medbotenv/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy in ./medbotenv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: sentence-transformers in ./medbotenv/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: pymupdf in ./medbotenv/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: openai in ./medbotenv/lib/python3.10/site-packages (1.61.0)\n",
      "Requirement already satisfied: pickle-mixin in ./medbotenv/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./medbotenv/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./medbotenv/lib/python3.10/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./medbotenv/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./medbotenv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./medbotenv/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./medbotenv/lib/python3.10/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in ./medbotenv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: jinja2 in ./medbotenv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./medbotenv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in ./medbotenv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./medbotenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./medbotenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./medbotenv/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: certifi in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./medbotenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./medbotenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./medbotenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./medbotenv/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./medbotenv/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./medbotenv/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./medbotenv/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./medbotenv/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: networkx in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./medbotenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./medbotenv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./medbotenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./medbotenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./medbotenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./medbotenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./medbotenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./medbotenv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./medbotenv/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./medbotenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./medbotenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./medbotenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./medbotenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./medbotenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./medbotenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit faiss-cpu numpy sentence-transformers pymupdf openai pickle-mixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created with 3397 entries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Directory where embeddings are stored\n",
    "EMBEDDING_DIR = \"/home/harish/Agentic_AI/embeddings\"\n",
    "TEXT_CHUNKS_DIR = \"/home/harish/Agentic_AI/text_chunks\"  # Directory for text chunks\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Loads stored embeddings (.npy) from the embedding directory.\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    text_chunks = []  # This will hold the corresponding text chunks\n",
    "\n",
    "    # Get the list of .npy files in the EMBEDDING_DIR\n",
    "    files_found = [f for f in os.listdir(EMBEDDING_DIR) if f.endswith(\".npy\")]\n",
    "    \n",
    "    if not files_found:\n",
    "        raise FileNotFoundError(\"⚠️ No `.npy` embedding files found in the specified directory!\")\n",
    "\n",
    "    for file in files_found:\n",
    "        file_path = os.path.join(EMBEDDING_DIR, file)\n",
    "        \n",
    "        # Load the corresponding text chunks (assuming the file name matches)\n",
    "        text_file = file.replace(\".npy\", \"_chunks.pkl\")\n",
    "        text_path = os.path.join(TEXT_CHUNKS_DIR, text_file)\n",
    "        \n",
    "        # Load embeddings\n",
    "        try:\n",
    "            embed = np.load(file_path)\n",
    "            embeddings_list.append(embed)\n",
    "\n",
    "            # Load text chunks\n",
    "            with open(text_path, \"rb\") as f:\n",
    "                texts = pickle.load(f)\n",
    "                text_chunks.extend(texts)  # Append texts to list\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file}: {e}\")\n",
    "            continue  # Skip this file if there's an issue\n",
    "\n",
    "    if not embeddings_list:\n",
    "        raise ValueError(\"❌ No valid embeddings found. Check your `.npy` files!\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings_list)  # Stack embeddings into a single numpy array\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index for fast similarity search.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # Create FAISS index for L2 distance\n",
    "    index.add(embeddings)  # Add the embeddings to the FAISS index\n",
    "    return index\n",
    "\n",
    "# Load embeddings and create FAISS index\n",
    "try:\n",
    "    embeddings, text_chunks = load_embeddings()\n",
    "    faiss_index = create_faiss_index(embeddings)\n",
    "    print(\"✅ FAISS index created with\", embeddings.shape[0], \"entries.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "Distance: 0.8545056581497192\n",
      "and cardiovascular disease\n",
      "■Differential Diagnosis\n",
      "•\n",
      "Nondiabetic glycosuria (eg, Fanconi’s syndrome)\n",
      "•\n",
      "Diabetes insipidus\n",
      "•\n",
      "Acromegaly\n",
      "•\n",
      "Cushing’s disease or syndrome\n",
      "•\n",
      "Pheochromocytoma\n",
      "•\n",
      "Medications (eg, glucocorticoids, niacin)\n",
      "■Treatment\n",
      "•\n",
      "Insulin treatment is required\n",
      "•\n",
      "Patient education is crucial, emphasizing dietary management,\n",
      "intensive insulin therapy, self-monitoring of blood glucose, hypo-\n",
      "glycemia awareness, foot and eye care\n",
      "■Pearl\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 2:\n",
      "Distance: 0.9506320357322693\n",
      "•\n",
      "Cushing’s disease or syndrome\n",
      "•\n",
      "Pheochromocytoma\n",
      "•\n",
      "Medications (eg, glucocorticoids, niacin)\n",
      "•\n",
      "Severe insulin resistance syndromes\n",
      "•\n",
      "Altered mental status due to other cause\n",
      "■Treatment\n",
      "•\n",
      "Patient education is important, emphasizing dietary management,\n",
      "exercise, weight loss, self-monitoring of blood glucose, hypo-\n",
      "glycemia awareness, foot and eye care\n",
      "•\n",
      "Mild cases may be controlled initially with diet, exercise, and\n",
      "weight loss\n",
      "•\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 3:\n",
      "Distance: 0.965509295463562\n",
      "200\n",
      "Current Essentials of Medicine\n",
      "7\n",
      "Type 1 Diabetes Mellitus\n",
      "■Essentials of Diagnosis\n",
      "•\n",
      "Crisp onset, no family history\n",
      "•\n",
      "Polyuria, polydipsia, weight loss\n",
      "•\n",
      "Fasting plasma glucose ≥126 mg/dL; random plasma glucose\n",
      "≥200 mg/dL with symptoms; glycosuria\n",
      "•\n",
      "Associated with ketosis in untreated state; may present as med-\n",
      "ical emergency (diabetic ketoacidosis)\n",
      "•\n",
      "Long-term risks include retinopathy, nephropathy, neuropathy,\n",
      "and cardiovascular disease\n",
      "■Differential Diagnosis\n",
      "•\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 4:\n",
      "Distance: 0.974091649055481\n",
      "healing an in vivo study. Kafkas Üniversitesi Veteriner Fakültesi \n",
      "Dergisi, 9(1), 43-47. \n",
      "3. Omeroglu, S., Cam, M., & Erdoğan, D. (2003). Streptozotosin ile \n",
      "deneysel diyabet oluşturulmuş sıçanlardaki deri yaralarında, \n",
      "insülin \n",
      "benzeri \n",
      "büyüme \n",
      "faktörü-I’in \n",
      "lokalizasyonunun \n",
      "immünohistokimyasal olarak gösterilmesi. Düzce Tıp Fakültesi \n",
      "Dergisi, 5, 5-8. \n",
      "4. Kapan, M., Aslanmirza, M. Y., Karip, A. B., Bozkurt, Y., Evsen, M. S., \n",
      "Sak, E., & Öngören, A. U. (2008). Lokal fenitoin ve üre\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 5:\n",
      "Distance: 0.9979539513587952\n",
      "mellitus, must be excluded \n",
      "■Differential Diagnosis \n",
      "•\n",
      "Urinary tract infection\n",
      "•\n",
      "Diabetes mellitus\n",
      "•\n",
      "Congenital genitourinary anomalies\n",
      "•\n",
      "Constipation\n",
      "•\n",
      "Child abuse\n",
      "•\n",
      "Behavioral difﬁculties \n",
      "■Treatment \n",
      "•\n",
      "Therapy for causative medical problems\n",
      "•\n",
      "Support and positive reinforcement for children and families\n",
      "•\n",
      "Fluid restriction and bladder emptying before bedtime\n",
      "•\n",
      "Alarm systems effective, but may take weeks to work\n",
      "•\n",
      "Desmopressin works quickly but does not provide long-term control\n",
      "•\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_faiss_index(query, faiss_index, embeddings, text_chunks, k=5):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar embeddings to the query.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: The query string to search for\n",
    "    - faiss_index: The FAISS index object\n",
    "    - embeddings: All embeddings to compare against\n",
    "    - text_chunks: Corresponding text chunks to retrieve\n",
    "    - k: The number of nearest neighbors to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    - top_k_chunks: The top k most similar text chunks\n",
    "    \"\"\"\n",
    "    # Convert query to embedding using the same model (you may want to use the same SentenceTransformer model here)\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Perform the search on the FAISS index\n",
    "    distances, indices = faiss_index.search(np.array(query_embedding).astype(np.float32), k)\n",
    "    \n",
    "    # Retrieve the most similar text chunks\n",
    "    top_k_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    \n",
    "    return top_k_chunks, distances[0]\n",
    "\n",
    "# Example query search\n",
    "query = \"What are the treatments for diabetes?\"\n",
    "top_k_chunks, distances = search_faiss_index(query, faiss_index, embeddings, text_chunks)\n",
    "\n",
    "# Display results\n",
    "for idx, (chunk, dist) in enumerate(zip(top_k_chunks, distances)):\n",
    "    print(f\"Rank {idx + 1}:\")\n",
    "    print(f\"Distance: {dist}\")\n",
    "    print(chunk)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(top_k_chunks, distances, max_length=500):\n",
    "    \"\"\"\n",
    "    Display the search results with cleaned-up formatting and truncated text for better readability.\n",
    "    \n",
    "    Parameters:\n",
    "    - top_k_chunks: List of the most similar text chunks\n",
    "    - distances: List of the corresponding distances for each chunk\n",
    "    - max_length: Maximum length of the chunk to display before truncating\n",
    "    \"\"\"\n",
    "    for idx, (chunk, dist) in enumerate(zip(top_k_chunks, distances)):\n",
    "        # Clean up text formatting\n",
    "        chunk = clean_text_formatting(chunk)\n",
    "        \n",
    "        # Truncate chunk to make it more readable\n",
    "        truncated_chunk = chunk[:max_length] + (\"...\" if len(chunk) > max_length else \"\")\n",
    "        \n",
    "        # Display results with rank and distance\n",
    "        print(f\"Rank {idx + 1}:\")\n",
    "        print(f\"Distance: {dist:.4f}\")\n",
    "        print(f\"Excerpt: {truncated_chunk}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def clean_text_formatting(text):\n",
    "    \"\"\"\n",
    "    Cleans up the text formatting, removing unnecessary symbols and characters like bullet points.\n",
    "    \"\"\"\n",
    "    # Replace bullet points, symbols, and extra spaces with cleaner text\n",
    "    cleaned_text = text.replace(\"■\", \"\").replace(\"•\", \"\").replace(\"\\n\", \" \").strip()\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "Distance: 0.8545\n",
      "Excerpt: and cardiovascular disease Differential Diagnosis Nondiabetic glycosuria (eg, Fanconi’s syndrome) Diabetes insipidus Acromegaly Cushing’s disease or syndrome Pheochromocytoma Medications (eg, glucocorticoids, niacin) Treatment Insulin treatment is required Patient education is crucial, emphasizing dietary management, intensive insulin therapy, self-monitoring of blood glucose, hypo- glycemia awareness, foot and eye care Pearl\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 2:\n",
      "Distance: 0.9506\n",
      "Excerpt: Cushing’s disease or syndrome Pheochromocytoma Medications (eg, glucocorticoids, niacin) Severe insulin resistance syndromes Altered mental status due to other cause Treatment Patient education is important, emphasizing dietary management, exercise, weight loss, self-monitoring of blood glucose, hypo- glycemia awareness, foot and eye care Mild cases may be controlled initially with diet, exercise, and weight loss\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 3:\n",
      "Distance: 0.9655\n",
      "Excerpt: 200 Current Essentials of Medicine 7 Type 1 Diabetes Mellitus Essentials of Diagnosis Crisp onset, no family history Polyuria, polydipsia, weight loss Fasting plasma glucose ≥126 mg/dL; random plasma glucose ≥200 mg/dL with symptoms; glycosuria Associated with ketosis in untreated state; may present as med- ical emergency (diabetic ketoacidosis) Long-term risks include retinopathy, nephropathy, neuropathy, and cardiovascular disease Differential Diagnosis\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 4:\n",
      "Distance: 0.9741\n",
      "Excerpt: healing an in vivo study. Kafkas Üniversitesi Veteriner Fakültesi Dergisi, 9(1), 43-47. 3. Omeroglu, S., Cam, M., & Erdoğan, D. (2003). Streptozotosin ile deneysel diyabet oluşturulmuş sıçanlardaki deri yaralarında, insülin benzeri büyüme faktörü-I’in lokalizasyonunun immünohistokimyasal olarak gösterilmesi. Düzce Tıp Fakültesi Dergisi, 5, 5-8. 4. Kapan, M., Aslanmirza, M. Y., Karip, A. B., Bozkurt, Y., Evsen, M. S., Sak, E., & Öngören, A. U. (2008). Lokal fenitoin ve üre\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 5:\n",
      "Distance: 0.9980\n",
      "Excerpt: mellitus, must be excluded Differential Diagnosis Urinary tract infection Diabetes mellitus Congenital genitourinary anomalies Constipation Child abuse Behavioral difﬁculties Treatment Therapy for causative medical problems Support and positive reinforcement for children and families Fluid restriction and bladder emptying before bedtime Alarm systems effective, but may take weeks to work Desmopressin works quickly but does not provide long-term control\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query search (already done)\n",
    "query = \"What are the treatments for diabetes?\"\n",
    "top_k_chunks, distances = search_faiss_index(query, faiss_index, embeddings, text_chunks)\n",
    "\n",
    "# Display results with improved formatting\n",
    "display_search_results(top_k_chunks, distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss_index(query, faiss_index, embeddings, text_chunks, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant text chunks based on the query using FAISS.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: The user's query.\n",
    "    - faiss_index: The FAISS index to search for relevant embeddings.\n",
    "    - embeddings: The embeddings of the text chunks.\n",
    "    - text_chunks: The list of text chunks.\n",
    "    - top_k: The number of top relevant chunks to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - top_k_chunks: The most relevant text chunks based on the query.\n",
    "    - distances: The distances (similarity scores) of the retrieved chunks.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Perform search in FAISS index\n",
    "    distances, indices = faiss_index.search(np.array(query_embedding).astype(np.float32), top_k)\n",
    "\n",
    "    # Get the top-k relevant chunks based on the indices\n",
    "    top_k_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    \n",
    "    return top_k_chunks, distances[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./medbotenv/lib/python3.10/site-packages (0.3.17)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./medbotenv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./medbotenv/lib/python3.10/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./medbotenv/lib/python3.10/site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./medbotenv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in ./medbotenv/lib/python3.10/site-packages (from langchain) (0.3.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./medbotenv/lib/python3.10/site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./medbotenv/lib/python3.10/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./medbotenv/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./medbotenv/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./medbotenv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./medbotenv/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./medbotenv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./medbotenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-generativeai<0.9.0,>=0.8.0 (from langchain-google-genai)\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in ./medbotenv/lib/python3.10/site-packages (from langchain-google-genai) (0.3.33)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./medbotenv/lib/python3.10/site-packages (from langchain-google-genai) (2.10.6)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading google_api_python_client-2.160.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in ./medbotenv/lib/python3.10/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.29.3)\n",
      "Requirement already satisfied: tqdm in ./medbotenv/lib/python3.10/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in ./medbotenv/lib/python3.10/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.3.4)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./medbotenv/lib/python3.10/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./medbotenv/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./medbotenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.23.0)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: anyio in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.14.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./medbotenv/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./medbotenv/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.3.1)\n",
      "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_api_python_client-2.160.0-py2.py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.70.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: filetype, uritemplate, pyparsing, pyasn1, proto-plus, grpcio, googleapis-common-protos, rsa, pyasn1-modules, httplib2, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
      "Successfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.15 google-api-core-2.24.1 google-api-python-client-2.160.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.66.0 grpcio-1.70.0 grpcio-status-1.70.0 httplib2-0.22.0 langchain-google-genai-2.0.9 proto-plus-1.26.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.2.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (3.11.11)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (0.3.17)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (0.3.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (0.3.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./medbotenv/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./medbotenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./medbotenv/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./medbotenv/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./medbotenv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./medbotenv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./medbotenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./medbotenv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./medbotenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./medbotenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./medbotenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./medbotenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.16 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./medbotenv/lib/python3.10/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-google-genai\n",
    "!pip install langchain-community\n",
    "!pip install\n",
    "# pip install faiss-cpu  # or faiss-gpu if you're using GPU\n",
    "# pip install pandas\n",
    "!pip install python-dotenv\n",
    "# pip install streamlit\n",
    "# pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created with 3397 entries.\n",
      "Response from ChatGroq: content=\"Based on the provided context, here's what I know about Acute Liver Failure:\\n\\n* Acute Liver Failure is a severe liver injury that occurs in a person with previously normal liver function, associated with the development of hepatic encephalopathy and evidence of hepatic synthetic dysfunction.\\n* Patients often present with new-onset jaundice, anorexia, nausea, vomiting, flu-like symptoms, or altered mental status.\\n* The etiologies of Acute Liver Failure include:\\n\\t+ Acetaminophen overdose\\n\\t+ Idiosyncratic drug reaction\\n\\t+ Acute viral hepatitis\\n\\t+ Exposure to hepatotoxins\\n\\t+ Autoimmune hepatitis\\n\\t+ Wilson's disease\\n\\t+ Complications of pregnancy\\n\\t+ Vascular disorders\\n* The diagnosis of Acute Liver Failure is based on the presence of severe liver injury, hepatic encephalopathy, and evidence of hepatic synthetic dysfunction.\\n* The treatment of Acute Liver Failure involves:\\n\\t+ Prompt recognition of the condition\\n\\t+ Consideration of giving N-acetylcysteine to all cases of acute liver failure, not just patients with acetaminophen overdose\\n\\t+ Resuscitation and stabilization\\n\\t+ Transfer to a transplant center\\n* The prognosis of Acute Liver Failure depends on the etiology, rapidity of onset, degree of encephalopathy, and development of complications.\\n* Markedly abnormal liver function tests, including elevated bilirubin, AST/ALT > 1000, and elevated international normalized ratio, are indicative of Acute Liver Failure.\\n\\nOverall, Acute Liver Failure is a serious condition that requires prompt recognition and treatment to improve outcomes.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 654, 'total_tokens': 998, 'completion_time': 0.286666667, 'prompt_time': 0.089276252, 'queue_time': 0.024058818999999995, 'total_time': 0.375942919}, 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-fad08594-96b9-4b20-8f01-c4fef5ec7d04-0' usage_metadata={'input_tokens': 654, 'output_tokens': 344, 'total_tokens': 998}\n",
      "Response type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Answer from ChatGroq: Based on the provided context, here's what I know about Acute Liver Failure:\n",
      "\n",
      "* Acute Liver Failure is a severe liver injury that occurs in a person with previously normal liver function, associated with the development of hepatic encephalopathy and evidence of hepatic synthetic dysfunction.\n",
      "* Patients often present with new-onset jaundice, anorexia, nausea, vomiting, flu-like symptoms, or altered mental status.\n",
      "* The etiologies of Acute Liver Failure include:\n",
      "\t+ Acetaminophen overdose\n",
      "\t+ Idiosyncratic drug reaction\n",
      "\t+ Acute viral hepatitis\n",
      "\t+ Exposure to hepatotoxins\n",
      "\t+ Autoimmune hepatitis\n",
      "\t+ Wilson's disease\n",
      "\t+ Complications of pregnancy\n",
      "\t+ Vascular disorders\n",
      "* The diagnosis of Acute Liver Failure is based on the presence of severe liver injury, hepatic encephalopathy, and evidence of hepatic synthetic dysfunction.\n",
      "* The treatment of Acute Liver Failure involves:\n",
      "\t+ Prompt recognition of the condition\n",
      "\t+ Consideration of giving N-acetylcysteine to all cases of acute liver failure, not just patients with acetaminophen overdose\n",
      "\t+ Resuscitation and stabilization\n",
      "\t+ Transfer to a transplant center\n",
      "* The prognosis of Acute Liver Failure depends on the etiology, rapidity of onset, degree of encephalopathy, and development of complications.\n",
      "* Markedly abnormal liver function tests, including elevated bilirubin, AST/ALT > 1000, and elevated international normalized ratio, are indicative of Acute Liver Failure.\n",
      "\n",
      "Overall, Acute Liver Failure is a serious condition that requires prompt recognition and treatment to improve outcomes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Directory for embeddings and text chunks\n",
    "EMBEDDING_DIR = \"/home/harish/Agentic_AI/embeddings\"\n",
    "TEXT_CHUNKS_DIR = \"/home/harish/Agentic_AI/text_chunks\"\n",
    "\n",
    "# Load environment variables (API keys, etc.)\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize ChatGroq LLM\n",
    "llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"Llama3-8b-8192\", temperature=0)\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Loads stored embeddings (.npy) and text chunks from their respective directories.\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    text_chunks = []\n",
    "\n",
    "    files_found = [f for f in os.listdir(EMBEDDING_DIR) if f.endswith(\".npy\")]\n",
    "    if not files_found:\n",
    "        raise FileNotFoundError(\"⚠️ No `.npy` embedding files found!\")\n",
    "\n",
    "    for file in files_found:\n",
    "        file_path = os.path.join(EMBEDDING_DIR, file)\n",
    "        text_file = file.replace(\".npy\", \"_chunks.pkl\")\n",
    "        text_path = os.path.join(TEXT_CHUNKS_DIR, text_file)\n",
    "\n",
    "        try:\n",
    "            embed = np.load(file_path)\n",
    "            embeddings_list.append(embed)\n",
    "\n",
    "            with open(text_path, \"rb\") as f:\n",
    "                texts = pickle.load(f)\n",
    "                text_chunks.extend(texts)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings_list:\n",
    "        raise ValueError(\"❌ No valid embeddings found.\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings_list)  # Stack embeddings\n",
    "    return embeddings, text_chunks\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index for fast similarity search.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance metric for FAISS\n",
    "    index.add(embeddings)  # Add embeddings to FAISS\n",
    "    return index\n",
    "\n",
    "# Load embeddings and create FAISS index\n",
    "try:\n",
    "    embeddings, text_chunks = load_embeddings()\n",
    "    faiss_index = create_faiss_index(embeddings)\n",
    "    print(\"✅ FAISS index created with\", embeddings.shape[0], \"entries.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize your embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use the model that fits your use case\n",
    "\n",
    "def search_faiss_index(query, faiss_index, embeddings, text_chunks, k=5):\n",
    "    \"\"\"\n",
    "    Perform a search on the FAISS index for the most similar embeddings to the query.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding using the same model\n",
    "    query_embedding = embedding_model.encode([query])  # Make sure embedding_model is defined\n",
    "\n",
    "    # Perform the FAISS search\n",
    "    distances, indices = faiss_index.search(np.array(query_embedding).astype(np.float32), k)\n",
    "    \n",
    "    # Retrieve the most similar text chunks\n",
    "    top_k_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    \n",
    "    return top_k_chunks, distances[0]\n",
    "\n",
    "\n",
    "def create_chatgroq_prompt(top_k_chunks, query):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt for ChatGroq using retrieved chunks.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(top_k_chunks)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Answer the following question based on the provided context:\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Question: {input}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    formatted_prompt = prompt.format(context=context, input=query)\n",
    "    return formatted_prompt\n",
    "\n",
    "def query_chatgroq_with_context(query, faiss_index, embeddings, text_chunks, top_k=5):\n",
    "    \"\"\"\n",
    "    Query ChatGroq with the enhanced context retrieved from FAISS search.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve the relevant chunks from FAISS\n",
    "    top_k_chunks, distances = search_faiss_index(query, faiss_index, embeddings, text_chunks, k=top_k)\n",
    "\n",
    "    # Step 2: Prepare the prompt for ChatGroq\n",
    "    formatted_prompt = create_chatgroq_prompt(top_k_chunks, query)\n",
    "\n",
    "    # Step 3: Query ChatGroq with the enhanced prompt\n",
    "    response = llm.invoke(formatted_prompt)  # Use the correct method to invoke\n",
    "\n",
    "    # Debugging: Print the response and its type\n",
    "    print(f\"Response from ChatGroq: {response}\")\n",
    "    print(f\"Response type: {type(response)}\")  # Check the type of response\n",
    "\n",
    "    # Assuming the answer is stored in the 'content' attribute\n",
    "    return response.content  # Or adjust if a different attribute is used\n",
    "\n",
    "# Example query\n",
    "query = \"what do you know about Acute Liver Failure\"\n",
    "\n",
    "# Query ChatGroq with relevant context from FAISS\n",
    "answer = query_chatgroq_with_context(query, faiss_index, embeddings, text_chunks, top_k=5)\n",
    "\n",
    "print(f\"Answer from ChatGroq: {answer}\")\n",
    "\n",
    "#unstructure.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (0.16.17)\n",
      "Requirement already satisfied: chardet in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (4.13.1)\n",
      "Requirement already satisfied: emoji in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (2025.1.28)\n",
      "Requirement already satisfied: langdetect in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (3.12.1)\n",
      "Requirement already satisfied: backoff in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (0.29.0)\n",
      "Requirement already satisfied: wrapt in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (6.1.1)\n",
      "Requirement already satisfied: python-oxmsg in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: olefile in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured) (2025.1.31)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (44.0.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.11.0,>=2.10.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (2.10.6)\n",
      "Requirement already satisfied: pypdf>=4.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (5.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pydantic<2.11.0,>=2.10.3->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pydantic<2.11.0,>=2.10.3->unstructured-client->unstructured) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade unstructured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[local-inference] in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (0.16.17)\n",
      "Requirement already satisfied: chardet in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (5.2.0)\n",
      "Requirement already satisfied: filetype in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (0.4.27)\n",
      "Requirement already satisfied: lxml in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (5.3.0)\n",
      "Requirement already satisfied: nltk in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (3.9.1)\n",
      "Requirement already satisfied: requests in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (4.13.1)\n",
      "Requirement already satisfied: emoji in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (2025.1.28)\n",
      "Requirement already satisfied: langdetect in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (3.12.1)\n",
      "Requirement already satisfied: backoff in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (0.29.0)\n",
      "Requirement already satisfied: wrapt in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (6.1.1)\n",
      "Requirement already satisfied: python-oxmsg in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (1.1)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[local-inference])\n",
      "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[local-inference])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pandas in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (2.2.3)\n",
      "Collecting unstructured-inference>=0.8.6 (from unstructured[local-inference])\n",
      "  Downloading unstructured_inference-0.8.7-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-vision (from unstructured[local-inference])\n",
      "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting effdet (from unstructured[local-inference])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: networkx in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (3.4.2)\n",
      "Collecting openpyxl (from unstructured[local-inference])\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown (from unstructured[local-inference])\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pi-heif in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (0.21.0)\n",
      "Requirement already satisfied: pypdf in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (5.2.0)\n",
      "Collecting pdf2image (from unstructured[local-inference])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pikepdf (from unstructured[local-inference])\n",
      "  Downloading pikepdf-9.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting onnx (from unstructured[local-inference])\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting xlrd (from unstructured[local-inference])\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pypandoc (from unstructured[local-inference])\n",
      "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pdfminer.six in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured[local-inference]) (20240706)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[local-inference])\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from python-pptx>=1.0.1->unstructured[local-inference]) (11.1.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[local-inference])\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting python-multipart (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: huggingface-hub in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-inference>=0.8.6->unstructured[local-inference]) (0.28.1)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting onnxruntime>=1.17.0 (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting matplotlib (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-inference>=0.8.6->unstructured[local-inference]) (2.6.0)\n",
      "Collecting timm (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-inference>=0.8.6->unstructured[local-inference]) (4.48.2)\n",
      "Requirement already satisfied: scipy in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-inference>=0.8.6->unstructured[local-inference]) (1.15.1)\n",
      "Collecting pypdfium2 (from unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pdfminer.six->unstructured[local-inference]) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pdfminer.six->unstructured[local-inference]) (44.0.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured.pytesseract>=0.3.12->unstructured[local-inference]) (24.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from beautifulsoup4->unstructured[local-inference]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from dataclasses-json->unstructured[local-inference]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from dataclasses-json->unstructured[local-inference]) (0.9.0)\n",
      "Collecting torchvision (from effdet->unstructured[local-inference])\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[local-inference])\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[local-inference])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (2.24.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-cloud-vision->unstructured[local-inference]) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-cloud-vision->unstructured[local-inference]) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-cloud-vision->unstructured[local-inference]) (5.29.3)\n",
      "Requirement already satisfied: six>=1.9 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from html5lib->unstructured[local-inference]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from html5lib->unstructured[local-inference]) (0.5.1)\n",
      "Requirement already satisfied: click in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured[local-inference]) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured[local-inference]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from nltk->unstructured[local-inference]) (2024.11.6)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[local-inference])\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pandas->unstructured[local-inference]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pandas->unstructured[local-inference]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pandas->unstructured[local-inference]) (2025.1)\n",
      "Collecting Deprecated (from pikepdf->unstructured[local-inference])\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: olefile in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from python-oxmsg->unstructured[local-inference]) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured[local-inference]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured[local-inference]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from requests->unstructured[local-inference]) (2025.1.31)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (24.1.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.11.0,>=2.10.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (2.10.6)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from unstructured-client->unstructured[local-inference]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (1.17.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (4.9)\n",
      "Requirement already satisfied: anyio in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (0.14.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[local-inference])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from omegaconf>=2.0->effdet->unstructured[local-inference]) (6.0.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.17.0->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: sympy in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.6->unstructured[local-inference]) (1.13.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading fonttools-4.55.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from matplotlib->unstructured-inference>=0.8.6->unstructured[local-inference]) (3.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pydantic<2.11.0,>=2.10.3->unstructured-client->unstructured[local-inference]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pydantic<2.11.0,>=2.10.3->unstructured-client->unstructured[local-inference]) (2.27.2)\n",
      "Requirement already satisfied: safetensors in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from timm->unstructured-inference>=0.8.6->unstructured[local-inference]) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (3.17.0)\n",
      "Requirement already satisfied: jinja2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference>=0.8.6->unstructured[local-inference]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.6->unstructured[local-inference]) (0.21.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[local-inference]) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[local-inference]) (2.22)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (0.6.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference>=0.8.6->unstructured[local-inference])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/harish/Agentic_AI/medbotenv/lib/python3.10/site-packages (from jinja2->torch->unstructured-inference>=0.8.6->unstructured[local-inference]) (3.0.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading unstructured_inference-0.8.7-py3-none-any.whl (48 kB)\n",
      "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pikepdf-9.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=8ffa4f25fdf2f5e121cf7598c6be3d8dd4082c3ea4d692f660c16ebb0782a9c8\n",
      "  Stored in directory: /home/harish/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: flatbuffers, antlr4-python3-runtime, XlsxWriter, xlrd, unstructured.pytesseract, python-multipart, python-docx, pypdfium2, pypandoc, pdf2image, opencv-python, onnx, omegaconf, markdown, kiwisolver, humanfriendly, fonttools, et-xmlfile, Deprecated, cycler, contourpy, python-pptx, pikepdf, openpyxl, matplotlib, coloredlogs, pycocotools, onnxruntime, torchvision, timm, google-cloud-vision, unstructured-inference, effdet\n",
      "Successfully installed Deprecated-1.2.18 XlsxWriter-3.2.2 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 contourpy-1.3.1 cycler-0.12.1 effdet-0.4.1 et-xmlfile-2.0.0 flatbuffers-25.1.24 fonttools-4.55.8 google-cloud-vision-3.9.0 humanfriendly-10.0 kiwisolver-1.4.8 markdown-3.7 matplotlib-3.10.0 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.20.1 opencv-python-4.11.0.86 openpyxl-3.1.5 pdf2image-1.17.0 pikepdf-9.5.1 pycocotools-2.0.8 pypandoc-1.15 pypdfium2-4.30.1 python-docx-1.1.2 python-multipart-0.0.20 python-pptx-1.0.2 timm-1.0.14 torchvision-0.21.0 unstructured-inference-0.8.7 unstructured.pytesseract-0.3.13 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured[local-inference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 697/697 [12:29<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 750.21 seconds\n",
      "Embeddings saved for Current_Essentials_of_Medicine.pdf\n",
      "Text chunks saved for Current_Essentials_of_Medicine.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unstructured.partition.pdf import partition_pdf  # Use partition_pdf for PDF processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Set directory to save embeddings and text chunks\n",
    "EMBEDDING_DIR = \"/home/harish/Agentic_AI/embeddings\"\n",
    "TEXT_CHUNKS_DIR = \"/home/harish/Agentic_AI/text_chunks\"  # Directory for text chunks\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "# Load SBERT model\n",
    "def load_embedding_model():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the model before running the code\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# Function to extract text from PDFs using partition_pdf\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Partition the PDF document using partition_pdf (with \"fast\" strategy for extractable text)\n",
    "    elements = partition_pdf(pdf_file, strategy=\"fast\")  # Adjust strategy if needed (\"hi_res\", \"ocr_only\")\n",
    "    \n",
    "    # Extract the text content from the elements\n",
    "    document_text = [element.text for element in elements if hasattr(element, 'text')]\n",
    "    return document_text\n",
    "\n",
    "# Function to generate embeddings using SBERT (batch processing)\n",
    "def get_embeddings_batch(texts):\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True, batch_size=16)  # Batch processing\n",
    "    return embeddings\n",
    "\n",
    "# Function to process the PDF files\n",
    "def process_pdf(file_path):\n",
    "    # Read the PDF using partition_pdf\n",
    "    document = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    # If chunks are too large, further split them using RecursiveCharacterTextSplitter\n",
    "    all_chunks = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    for chunk in document:\n",
    "        sub_chunks = text_splitter.split_text(chunk)\n",
    "        all_chunks.extend(sub_chunks)\n",
    "    \n",
    "    # Batch processing of embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = get_embeddings_batch(all_chunks)\n",
    "    print(f\"Embedding generation took {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Save embeddings to .npy file\n",
    "    save_path = os.path.join(EMBEDDING_DIR, f\"{os.path.basename(file_path)}.npy\")\n",
    "    np.save(save_path, embeddings)\n",
    "    print(f\"Embeddings saved for {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Save text chunks to .pkl file\n",
    "    text_chunks_path = os.path.join(TEXT_CHUNKS_DIR, f\"{os.path.basename(file_path)}_chunks.pkl\")\n",
    "    with open(text_chunks_path, 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"Text chunks saved for {os.path.basename(file_path)}\")\n",
    "\n",
    "# Process PDF files (adjust the paths to the PDFs)\n",
    "pdf_files = [\"/home/harish/Agentic_AI/books/Current_Essentials_of_Medicine.pdf\"]  # Example PDF file paths\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 167/167 [02:45<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 165.52 seconds\n",
      "Embeddings saved for Current_Essentials_of_Medicine.pdf\n",
      "Text chunks saved for Current_Essentials_of_Medicine.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unstructured.partition.pdf import partition_pdf  # Use partition_pdf for PDF processing\n",
    "from unstructured.chunking.title import chunk_by_title  # Import chunking strategy (by title or by similarity)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Set directory to save embeddings and text chunks\n",
    "EMBEDDING_DIR = \"/home/harish/Agentic_AI/embeddings\"\n",
    "TEXT_CHUNKS_DIR = \"/home/harish/Agentic_AI/text_chunks\"  # Directory for text chunks\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_CHUNKS_DIR, exist_ok=True)\n",
    "\n",
    "# Load SBERT model\n",
    "def load_embedding_model():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the model before running the code\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# Function to extract text from PDFs using partition_pdf\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Partition the PDF document using partition_pdf (with \"fast\" strategy for extractable text)\n",
    "    elements = partition_pdf(pdf_file, strategy=\"fast\")  # Adjust strategy if needed (\"hi_res\", \"ocr_only\")\n",
    "    \n",
    "    # Extract the text content from the elements\n",
    "    document_text = [element.text for element in elements if hasattr(element, 'text')]\n",
    "    return document_text\n",
    "\n",
    "# Function to chunk text based on title or similarity\n",
    "def chunk_text(elements, strategy=\"by_title\"):\n",
    "    # Choose the chunking strategy (by title or by similarity)\n",
    "    if strategy == \"by_title\":\n",
    "        from unstructured.chunking.title import chunk_by_title\n",
    "        chunks = chunk_by_title(elements)\n",
    "    elif strategy == \"by_similarity\":\n",
    "        from unstructured.chunking.basic import chunk_elements\n",
    "        chunks = chunk_elements(elements, strategy=\"by_similarity\", similarity_threshold=0.7)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown chunking strategy. Choose 'by_title' or 'by_similarity'.\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to generate embeddings using SBERT (batch processing)\n",
    "def get_embeddings_batch(texts):\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True, batch_size=16)  # Batch processing\n",
    "    return embeddings\n",
    "\n",
    "# Function to process the PDF files\n",
    "def process_pdf(file_path, chunking_strategy=\"by_title\"):\n",
    "    # Read the PDF using partition_pdf\n",
    "    elements = partition_pdf(file_path, strategy=\"fast\")  # Use partition_pdf to extract text\n",
    "    \n",
    "    # Chunk the extracted elements by title or similarity\n",
    "    chunks = chunk_text(elements, strategy=chunking_strategy)\n",
    "    \n",
    "    # If chunks are too large, further split them using RecursiveCharacterTextSplitter\n",
    "    all_chunks = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    for chunk in chunks:\n",
    "        sub_chunks = text_splitter.split_text(chunk.text)\n",
    "        all_chunks.extend(sub_chunks)\n",
    "    \n",
    "    # Batch processing of embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = get_embeddings_batch(all_chunks)\n",
    "    print(f\"Embedding generation took {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Save embeddings to .npy file\n",
    "    save_path = os.path.join(EMBEDDING_DIR, f\"{os.path.basename(file_path)}.npy\")\n",
    "    np.save(save_path, embeddings)\n",
    "    print(f\"Embeddings saved for {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Save text chunks to .pkl file\n",
    "    text_chunks_path = os.path.join(TEXT_CHUNKS_DIR, f\"{os.path.basename(file_path)}_chunks.pkl\")\n",
    "    with open(text_chunks_path, 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"Text chunks saved for {os.path.basename(file_path)}\")\n",
    "\n",
    "# Process PDF files (adjust the paths to the PDFs)\n",
    "pdf_files = [\"/home/harish/Agentic_AI/books/Current_Essentials_of_Medicine.pdf\"]  # Example PDF file paths\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file, chunking_strategy=\"by_title\")  # Use either chunking_strategy'by_title' or 'by_similarity'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
